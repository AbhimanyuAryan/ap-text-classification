{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/josebambora/mistral-sentimental-analysis?scriptVersionId=177657546\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Introduction\n\nThis notebook has contains the process for tuning Mistral LLM model for sentimental analysis task. \n\nThis notebook was based on a public notebook found [Luca Massaron](https://www.kaggle.com/code/lucamassaron/fine-tune-mistral-v0-2-for-sentiment-analysis).\n\nThis notebook was develop for a university project. The main goal of it is to use different LLM models, tune them when possible and see their performance for sentimental analysis task.","metadata":{}},{"cell_type":"markdown","source":"## Installs\n\nInitially, we installed specific versions of some required packages. The packages, their respective versions, and their purposes that we used are listed in the following table:\n\n| Package       | Version         | Purpose                                                                |\n|---------------|-----------------|------------------------------------------------------------------------|\n| torch         | 2.0.0           | Loading and tuning the model on the GPU.                               |\n| accelerate    | 0.25.0          | Optimizing the tuning process.                                         |\n| peft          | 0.7.1           | Package with helpful configurations for tuning Mistral.                |\n| bitsandbytes  | 0.41.3.post2   | Applying quantization to reduce model size.                             |\n| transformers  | 4.36.1          | Package with public Hugging Face models, such as Mistral.               |\n| trl           | 0.7.4           | Debugging package used in the predict function. It simply prints the progress of a for loop. |\n| datasets      | latest          | Package to load public datasets on Hugging Face, such as IMDB.         |","metadata":{}},{"cell_type":"code","source":"!pip install -q -U torch=='2.0.0'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U accelerate=='0.25.0' peft=='0.7.1' bitsandbytes=='0.41.3.post2' transformers=='4.36.1' trl=='0.7.4'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports\n\nHere we realize the imports of the necessary libraries.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset, concatenate_datasets\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom transformers import (AutoModelForCausalLM,\n                          AutoTokenizer,\n                          BitsAndBytesConfig,\n                          TrainingArguments,\n                          pipeline,\n                          logging,\n                         TrainerCallback)\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom datasets import load_dataset, load_metric\nimport re\nimport requests\nimport gzip\nimport shutil\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hugging Face Login\n\nLogin in Hugging face in order to acess Mistral LLM model.","metadata":{}},{"cell_type":"code","source":"notebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data collection and preparation\n\nTo tune Mistral we are going to use the [IMDb dataset](https://huggingface.co/datasets/stanfordnlp/imdb). To test, our [test data](https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz) was gotten from a public repository in github.\n\nThe function *save_data* gets [test data](https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz) and stores it locally.","metadata":{}},{"cell_type":"code","source":"def save_data():\n    url = \"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz\"\n    filename = url.split(\"/\")[-1]\n\n    with open(filename, \"wb\") as f:\n        r = requests.get(url)\n        f.write(r.content)\n\n    with gzip.open('movie_data.csv.gz', 'rb') as f_in:\n        with open('movie_data.csv', 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\nsave_data()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each instruction passed to Mistral must begin with the command [INST] and end with the command [/INST]. Due to this requirement, we created the functions *generate_prompt* and *generate_test_prompt*. These functions are applied to every element in the training data and test data, respectively. In both functions, we defined a phrase to provide Mistral with context regarding what it needs to learn.","metadata":{}},{"cell_type":"code","source":"def generate_prompt(data_point):\n    label = 'positive'\n    if data_point[\"label\"] != 1:\n        label = 'negative'\n    res = f\"\"\"\n            [INST]Analyze the sentiment of the movie review enclosed in square brackets,\n            determine if it is positive, or negative, and return the answer as\n            the corresponding sentiment label \"positive\" or \"negative\"[/INST]\n\n            [{data_point[\"text\"]}] = {label}\"\"\".strip()\n    return re.sub(r'\\s+', ' ', res)\n\ndef generate_test_prompt(data_point):\n    res = f\"\"\"\n            [INST]Analyze the sentiment of the movie review enclosed in square brackets,\n            determine if it is positive, or negative, and return the answer as\n            the corresponding sentiment label \"positive\" or \"negative\"[/INST]\n\n            [{data_point}] = \"\"\".strip()\n    return re.sub(r'\\s+', ' ', res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Afterward, we defined the *select* and *generate_data* functions. The select function filters the original data passed as an argument and returns only a few elements from the filtered data. We did this to reduce the dataset size and consequently decrease training times. Another important reason for this function is to maintain data balance. The *generate_data* function simply applies the *generate_prompt* function to train the data.","metadata":{}},{"cell_type":"code","source":"random_seed = 2000\n\ndef select(data,label_result,range_num):\n    return data.filter(lambda example: example['label'] == label_result).shuffle(seed=random_seed).select(range(range_num))\n\ndef generate_data(data):\n    return data.shuffle(seed=random_seed).map(lambda elem : {'text': generate_prompt(elem)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The final step related to processing the data is the function *prepare_data*. This function will call the function *prepare_data_train*, in which we will process the data for training, as the name suggests. Here, we obtain both training data and validation data. Essentially, we extract 380 cases from the original dataset, of which 280 will be used for training and 100 for validation. It is important to mention that we preserved the data balance, ensuring an equal number of positive and negative reviews in both sets. The other component is used solely to test the model before and after training. For this purpose, we followed the same procedure as we did for *DistilBERT* so that we can compare model performances using the same test data. In order to see of our model is able to make some prediction, we also select 100 cases from the unsupervised imdb reviews and make classifications for them with our tuned model.","metadata":{}},{"cell_type":"code","source":"def prepare_data_train(imdb,train_size,eval_size,unsupervized_size):\n    aux = int((train_size + eval_size) / 2)\n    positive_rows = select(imdb['train'],1,aux)\n    negative_rows = select(imdb['train'],0,aux)\n    \n    half_train_size = int(train_size / 2)\n    half_eval_size = int(eval_size / 2)\n    \n    positive_rows_train = positive_rows.select(range(half_train_size))\n    negative_rows_train = negative_rows.select(range(half_train_size))\n    positive_rows_eval  = positive_rows.select(range(half_train_size, half_train_size + half_eval_size))\n    negative_rows_eval  = negative_rows.select(range(half_train_size, half_train_size + half_eval_size))\n    \n    selected_rows_train = concatenate_datasets([positive_rows_train, negative_rows_train])\n    selected_rows_eval  = concatenate_datasets([positive_rows_eval, negative_rows_eval])\n    \n    data_train = generate_data(selected_rows_train)\n    data_eval  = generate_data(selected_rows_eval)\n    \n    unsupervised_data = pd.DataFrame(imdb['unsupervised'].select(range(unsupervized_size)))\n    unsupervised_data['original'] = unsupervised_data['text']\n    unsupervised_data['text'] = unsupervised_data['text'].apply(generate_test_prompt)\n    return data_train,data_eval, unsupervised_data.drop(['label'],axis=1)\n\ndef prepare_data_test(df,test_size):\n    limsup = 40000 + test_size\n    X_test = df.iloc[40000:limsup]\n    X_test['text'] = X_test['review']\n    X_test['text'] = X_test['text'].apply(generate_test_prompt)\n    y_true = list(X_test['sentiment'])\n    return X_test.drop(['sentiment','review'],axis=1), y_true\n\ndef prepare_data(train_size,eval_size,test_size,unsupervized_size):\n    df = pd.read_csv('movie_data.csv')\n    imdb = load_dataset('imdb')\n    data_train, data_eval, unsupervized_data = prepare_data_train(imdb,train_size,eval_size,unsupervized_size)\n    X_test, y_true = prepare_data_test(df,test_size)\n    return data_train, data_eval, X_test, y_true, unsupervized_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train, data_eval, X_test, y_true, unsupervized_data = prepare_data(280,100,2500,100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Debug Messages, comment if not necessary\nprint(data_train)\nprint(data_eval)\nprint(unsupervized_data.info())\nprint(X_test.info())\nprint(len(y_true))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions for Model Evaluation\n\n\nBefore obtaining the model itself and tuning it, we need to define a method for evaluating it. For that purpose, we have defined multiple functions.\n\nFirstly, we have the functions *accuracy_for_label* and *evaluate*. As their names suggest, the first function returns the accuracy of model answers for each classification (positive or negative), while the second function evaluates the overall model responses.\n\nThe metrics we have utilized include overall accuracy and accuracy for each label (positive and negative reviews).","metadata":{}},{"cell_type":"code","source":"def accuracy_for_label(y_true, y_pred, label):\n    label_indices = [i for i, y in enumerate(y_true) if y == label]\n    label_y_true = [y_true[i] for i in label_indices]\n    label_y_pred = [y_pred[i] for i in label_indices]\n    accuracy = accuracy_score(label_y_true, label_y_pred)\n    return accuracy,int(len(label_y_true)*accuracy),len(label_y_true)\n\ndef evaluate(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Overall Accuracy\n    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n    print(f'Accuracy: {accuracy:.3f}')\n\n    # Accuracy for each label\n    accuracy_negative,correct_negative,total_negative = accuracy_for_label(y_true,y_pred,0)\n    accuracy_positive,correct_positive,total_positive = accuracy_for_label(y_true,y_pred,1)\n\n    print(f'Accuracy for negative reviews: {accuracy_negative:.3f} ({correct_negative},{total_negative})')\n    print(f'Accuracy for positive reviews: {accuracy_positive:.3f} ({correct_positive},{total_positive})')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions for Answer Generation\n\nNext, we have created the functions *generate_response* and *predict*. As their names suggest, the first function provides a single prompt to the model and returns its response. The second function provides all the test prompts to the model and saves the model's responses. We obtain the model's response using the pipeline package. Specifically, we specify that we only want responses with 1 token, so that the model provides only one word. If the model generates a word different from \"positive,\" we consider that the model has evaluated the review as negative, regardless of whether it responded with \"negative\" or something else.","metadata":{}},{"cell_type":"code","source":"def generate_response(prompt,model,tokenizer):\n    pipe = pipeline(task=\"text-generation\",\n                        model=model,\n                        tokenizer=tokenizer,\n                        max_new_tokens = 1,\n                        temperature = 0.0)\n    result = pipe(prompt, pad_token_id=pipe.tokenizer.eos_token_id)\n    return result[0]['generated_text'].split(\"=\")[-1].lower()\n\ndef predict(X_test, model, tokenizer):\n    y_pred = []\n    for i in tqdm(range(len(X_test))):\n        prompt = X_test.iloc[i][\"text\"]\n        answer = generate_response(prompt,model,tokenizer)\n        if \"positive\" in answer:\n            y_pred.append(1)\n        else:\n            y_pred.append(0)\n    return y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mistral Workflow\n\nWe have created a function to retrieve the Mistral model and its tokenizer. The used model is the instruct second version of Mistral ([Model Page](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)).\n\nAn important aspect we've utilized is the package [BitsAndBytesConfig](https://huggingface.co/docs/transformers/main_classes/quantization). This package is useful for configuring the model itself for quantization. Quantization, in general, is a process of reducing the number of bits used to represent data while preserving essential information, resulting in a reduction of model size and an increase in speed (Dettmers, Tim, and Luke Zettlemoyer. 2023. “The Case for 4-Bit Precision: K-Bit Inference Scaling Laws.” https://arxiv.org/abs/2212.09720). Regarding the arguments, *load_in_4bit* is set to True to enable 4-bit quantization. This replaces the Linear layers with FP4/NF4 layers from `bitsandbytes`. The argument *bnb_4bit_use_double_quant* is set to False to prevent re-quantization of quantization constants. *bnb_4bit_quant_type* is set to \"nf4\", where \"nf4\" stands for \"Non-Flowing 4-bit\". This means the model will map the 4-bit quantized values to a discrete set of integers without any fractional parts. Lastly, *bnb_4bit_compute_dtype* is set to `getattr(torch, \"float16\")` to specify the computational type as float bits, thereby increasing computation speed.\n\nWe have used 4 bits instead of 8 bits mainly to reduce the memory size, since at the beginning of the project we were encountering CUDA out of memory errors multiple times. Obviously, this brings some problems such as losing accuracy and precision with the values, as well as limiting the range and 4-bit representations are more susceptible to noise. As for the advantages, the memory size is reduced. It also reduces bandwidth consumption, speeds up computation, and is more energy-efficient.\n\nAfter defining the `BitsAndBytesConfig` object, we retrieve the model itself by passing its name, the `BitsAndBytesConfig` object, and setting *device_map* to \"auto\" so that the model is loaded onto the GPU. Finally, we obtain the model's tokenizer.","metadata":{}},{"cell_type":"code","source":"def get_model():\n    model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n    compute_dtype = getattr(torch, \"float16\")\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=False,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        quantization_config=bnb_config,\n    )\n    model.config.use_cache = False\n    model.config.pretraining_tp = 1\n    tokenizer = AutoTokenizer.from_pretrained(model_name,\n                                              trust_remote_code=True,\n                                              padding_side=\"left\",\n                                              add_bos_token=True,\n                                              add_eos_token=True,\n                                            )\n    tokenizer.pad_token = tokenizer.eos_token\n    return (model,tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model,tokenizer = get_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Base Model Performance. Since this evaluation takes too much time, it is in comments, but uncomment if necessary.\n# y_pred = predict(X_test, model, tokenizer)\n# evaluate(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this point, we've created a function responsible for creating an object that we will use to train the model.\n\nFirstly, we have instantiated a `LoraConfig` object. [Lora](https://huggingface.co/docs/peft/v0.8.0/en/package_reference/lora) stands for Low-Rank Adaptation, and it is a PEFT method that decomposes a large matrix into two smaller low-rank matrices in the attention layers. This results in a drastic reduction in the number of parameters that need to be fine-tuned. Regarding the parameters, we've set *lora_alpha* to 16, indicating that the LoRa regularization is set to 16. The *lora_dropout* parameter is set to 0.1, which helps prevent the model from overfitting. The parameter *r* represents the number of multi-head attention the model will use. The *bias* parameter determines whether biases are included in the model's computations. In our case, it is set to None since we do not have biases. The last argument, *task_type*, specifies the type of task the model is trained for, which is set to causal language modeling. This means that the task the model will be trained for is to predict the next word in a sequence given the previous words.\n\nRegarding the [TrainingArguments](https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.TrainingArguments), the first one is *output_dir*, which is simply a Hugging Face project where we want to save the retrained model. Then, we specify the number of epochs (4), the batch size (1), the gradient accumulation (4), the optimizer to use (paged_adamw_32bit), the number of steps to save the checkpoint (0), the number of steps between two logs (25), the learning rate ($2 \\times 10^{-4}$), the weight decay to apply to all layers except all bias and LayerNorm weights (0.001), the usage of floating-point 16-bit values, maximum gradient norm (0.3), the ratio of total training steps used for a linear warmup from 0 to *learning_rate* (0.03), to group together samples of roughly the same length in the training dataset in order to minimize padding applied and be more efficient, the scheduler type to use (cosine), and finally, evaluation is done at the end of each epoch.\n\nAt the end, we use the [Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer) (SFTT), which will be the trainer object. We pass as arguments our model, the training data, the evaluation data (so that the training process can calculate validation values), the *Lora configuration*, the model *tokenizer*, the training arguments, and we set that the sequences have a maximum length of 512. The reasons why we used this type of trainer instead of the traditional [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) are:\n\n1. This type is more appropriated to text classification problems\n2. Our dataset is not that large. \n3. The training process is faster, \n4. Uses less memory.","metadata":{}},{"cell_type":"code","source":"def train_configuration():\n    peft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    training_arguments = TrainingArguments(\n        output_dir=\"mistral_retrained\",\n        num_train_epochs=4,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        optim=\"paged_adamw_32bit\",\n        save_steps=0,\n        logging_steps=25,\n        learning_rate=2e-4,\n        weight_decay=0.001,\n        fp16=True,\n        max_grad_norm=0.3,\n        warmup_ratio=0.03,\n        group_by_length=True,\n        lr_scheduler_type=\"cosine\",\n        report_to=\"tensorboard\",\n        evaluation_strategy=\"epoch\"\n    )\n\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=data_train,\n        eval_dataset=data_eval,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        tokenizer=tokenizer,\n        args=training_arguments,\n        packing=False,\n        max_seq_length=512\n    )\n    return trainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning\n\nSo now we retrain Mistral with our data. The training process took 1 hour and 26 minutes.","metadata":{}},{"cell_type":"code","source":"trainer = train_configuration()\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation\n\nNow we need to evaluate our fine tuned Mistral. The evaluation took 1 hour 20 minutes.","metadata":{}},{"cell_type":"code","source":"y_pred = predict(X_test, model, tokenizer)\nevaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.idle":"2024-05-14T17:08:48.83882Z","shell.execute_reply.started":"2024-05-14T15:47:33.54538Z","shell.execute_reply":"2024-05-14T17:08:48.837797Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"100%|██████████| 2500/2500 [1:21:15<00:00,  1.95s/it]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.961\nAccuracy for negative reviews: 0.976 (1237,1268)\nAccuracy for positive reviews: 0.946 (1165,1232)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Unsupervised\n\nNow we will show how we can use our fine tuned Mistral.","metadata":{}},{"cell_type":"code","source":"def make_predictions(unsupervized_data,model,tokenizer):\n    predictions = predict(unsupervized_data,model,tokenizer)\n    classification = unsupervized_data.copy()\n    classification.drop(['text'],axis=1,inplace=True)\n    classification['label'] = predictions\n    return classification","metadata":{"execution":{"iopub.status.busy":"2024-05-14T17:08:48.840199Z","iopub.execute_input":"2024-05-14T17:08:48.840536Z","iopub.status.idle":"2024-05-14T17:08:48.845969Z","shell.execute_reply.started":"2024-05-14T17:08:48.840507Z","shell.execute_reply":"2024-05-14T17:08:48.845038Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"predictions = make_predictions(unsupervized_data,model,tokenizer)\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T17:08:48.84704Z","iopub.execute_input":"2024-05-14T17:08:48.847301Z","iopub.status.idle":"2024-05-14T17:11:36.003013Z","shell.execute_reply.started":"2024-05-14T17:08:48.847277Z","shell.execute_reply":"2024-05-14T17:11:36.002105Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [02:47<00:00,  1.67s/it]","output_type":"stream"},{"name":"stdout","text":"                                             original  label\n0   This is just a precious little diamond. The pl...      1\n1   When I say this is my favourite film of all ti...      1\n2   I saw this movie because I am a huge fan of th...      1\n3   Being that the only foreign films I usually li...      0\n4   After seeing Point of No Return (a great movie...      0\n..                                                ...    ...\n95  I'm surprised how many people see this as a bo...      1\n96  With the movie market being so saturated with ...      1\n97  I enjoyed this movie. I rented it because it h...      1\n98  Many of you have posted comments on \"Clockwatc...      1\n99  That is exactly what I did while watching this...      0\n\n[100 rows x 2 columns]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Publish\n\nPublish this fine tuned model to hugging face","metadata":{}},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T17:13:33.843444Z","iopub.execute_input":"2024-05-14T17:13:33.844387Z","iopub.status.idle":"2024-05-14T17:13:46.962277Z","shell.execute_reply.started":"2024-05-14T17:13:33.844353Z","shell.execute_reply":"2024-05-14T17:13:46.961365Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/109M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38fd0389d4534a288550ed7688397a46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.28k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3eee18017694c8da97f9e256aea77b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"779ba527c4464664b4a0e027143d3f5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1715699938.089a828f3b46.34.0:   0%|          | 0.00/7.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63b3ac78032e4b07bd59e112a5d89495"}},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/JoseBambora/mistral_retrained/commit/4804a9658d9e87fb23d2c54692281612ced9b4d2', commit_message='End of training', commit_description='', oid='4804a9658d9e87fb23d2c54692281612ced9b4d2', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}