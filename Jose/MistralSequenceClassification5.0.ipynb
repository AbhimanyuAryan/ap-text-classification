{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/josebambora/mistral-sentimental-analysis?scriptVersionId=177584255\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Introduction\n\nThis notebook has contains the process for tuning Mistral LLM model for sentimental analysis task. \n\nThis notebook was based on a public notebook found [Luca Massaron](https://www.kaggle.com/code/lucamassaron/fine-tune-mistral-v0-2-for-sentiment-analysis).\n\nThis notebook was develop for a university project. The main goal of it is to use different LLM models, tune them when possible and see their performance for sentimental analysis task.","metadata":{}},{"cell_type":"markdown","source":"## Installs\n\nInitially, we installed specific versions of some required packages. The packages, their respective versions, and their purposes that we used are listed in the following table:\n\n| Package       | Version         | Purpose                                                                |\n|---------------|-----------------|------------------------------------------------------------------------|\n| torch         | 2.0.0           | Loading and tuning the model on the GPU.                               |\n| accelerate    | 0.25.0          | Optimizing the tuning process.                                         |\n| peft          | 0.7.1           | Package with helpful configurations for tuning Mistral.                |\n| bitsandbytes  | 0.41.3.post2   | Applying quantization to reduce model size.                             |\n| transformers  | 4.36.1          | Package with public Hugging Face models, such as Mistral.               |\n| trl           | 0.7.4           | Debugging package used in the predict function. It simply prints the progress of a for loop. |\n| datasets      | latest          | Package to load public datasets on Hugging Face, such as IMDB.         |","metadata":{}},{"cell_type":"code","source":"!pip install -q -U torch=='2.0.0'","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:58:00.441542Z","iopub.execute_input":"2024-05-14T09:58:00.442425Z","iopub.status.idle":"2024-05-14T10:00:09.125344Z","shell.execute_reply.started":"2024-05-14T09:58:00.442388Z","shell.execute_reply":"2024-05-14T10:00:09.124031Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U accelerate=='0.25.0' peft=='0.7.1' bitsandbytes=='0.41.3.post2' transformers=='4.36.1' trl=='0.7.4'","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:00:09.127728Z","iopub.execute_input":"2024-05-14T10:00:09.128055Z","iopub.status.idle":"2024-05-14T10:00:39.678447Z","shell.execute_reply.started":"2024-05-14T10:00:09.128028Z","shell.execute_reply":"2024-05-14T10:00:39.677173Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Imports\n\nHere we realize the imports of the necessary libraries.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login, login\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset, concatenate_datasets\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom transformers import (AutoModelForCausalLM,\n                          AutoTokenizer,\n                          BitsAndBytesConfig,\n                          TrainingArguments,\n                          pipeline,\n                          logging,\n                         TrainerCallback)\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom datasets import load_dataset, load_metric\nimport re\nimport requests\nimport gzip\nimport shutil\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:00:39.680022Z","iopub.execute_input":"2024-05-14T10:00:39.680328Z","iopub.status.idle":"2024-05-14T10:00:59.945096Z","shell.execute_reply.started":"2024-05-14T10:00:39.6803Z","shell.execute_reply":"2024-05-14T10:00:59.943835Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-14 10:00:48.848126: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-14 10:00:48.848224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-14 10:00:48.974306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Hugging Face Login\n\nLogin in Hugging face in order to acess Mistral LLM model.","metadata":{}},{"cell_type":"code","source":"notebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:02:07.011753Z","iopub.execute_input":"2024-05-14T10:02:07.012134Z","iopub.status.idle":"2024-05-14T10:02:07.034511Z","shell.execute_reply.started":"2024-05-14T10:02:07.012106Z","shell.execute_reply":"2024-05-14T10:02:07.033742Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bfcd01c892a4a4b98cfcc82440ee312"}},"metadata":{}}]},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-05-14T10:04:20.97852Z","iopub.execute_input":"2024-05-14T10:04:20.979521Z","iopub.status.idle":"2024-05-14T10:04:20.983567Z","shell.execute_reply.started":"2024-05-14T10:04:20.979486Z","shell.execute_reply":"2024-05-14T10:04:20.98289Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Data collection and preparation\n\nTo tune Mistral we are going to use the [IMDb dataset](https://huggingface.co/datasets/stanfordnlp/imdb). To test, our [test data](https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz) was gotten from a public repository in github.\n\nThe function *save_data* gets [test data](https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz) and stores it locally.","metadata":{}},{"cell_type":"code","source":"def save_data():\n    url = \"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz\"\n    filename = url.split(\"/\")[-1]\n\n    with open(filename, \"wb\") as f:\n        r = requests.get(url)\n        f.write(r.content)\n\n    with gzip.open('movie_data.csv.gz', 'rb') as f_in:\n        with open('movie_data.csv', 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\nsave_data()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T08:58:29.599365Z","iopub.execute_input":"2024-05-14T08:58:29.600096Z","iopub.status.idle":"2024-05-14T08:58:30.792231Z","shell.execute_reply.started":"2024-05-14T08:58:29.600065Z","shell.execute_reply":"2024-05-14T08:58:30.791388Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Each instruction passed to Mistral must begin with the command [INST] and end with the command [/INST]. Due to this requirement, we created the functions *generate_prompt* and *generate_test_prompt*. These functions are applied to every element in the training data and test data, respectively. In both functions, we defined a phrase to provide Mistral with context regarding what it needs to learn.","metadata":{}},{"cell_type":"code","source":"def generate_prompt(data_point):\n    label = 'positive'\n    if data_point[\"label\"] != 1:\n        label = 'negative'\n    res = f\"\"\"\n            [INST]Analyze the sentiment of the movie review enclosed in square brackets,\n            determine if it is positive, or negative, and return the answer as\n            the corresponding sentiment label \"positive\" or \"negative\"[/INST]\n\n            [{data_point[\"text\"]}] = {label}\"\"\".strip()\n    return re.sub(r'\\s+', ' ', res)\n\ndef generate_test_prompt(data_point):\n    res = f\"\"\"\n            [INST]Analyze the sentiment of the movie review enclosed in square brackets,\n            determine if it is positive, or negative, and return the answer as\n            the corresponding sentiment label \"positive\" or \"negative\"[/INST]\n\n            [{data_point}] = \"\"\".strip()\n    return re.sub(r'\\s+', ' ', res)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T08:58:33.417255Z","iopub.execute_input":"2024-05-14T08:58:33.417628Z","iopub.status.idle":"2024-05-14T08:58:33.423973Z","shell.execute_reply.started":"2024-05-14T08:58:33.417599Z","shell.execute_reply":"2024-05-14T08:58:33.423132Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Afterward, we defined the *select* and *generate_data* functions. The select function filters the original data passed as an argument and returns only a few elements from the filtered data. We did this to reduce the dataset size and consequently decrease training times. Another important reason for this function is to maintain data balance. The *generate_data* function simply applies the *generate_prompt* function to train the data.","metadata":{}},{"cell_type":"code","source":"random_seed = 2000\n\ndef select(data,label_result,range_num):\n    return data.filter(lambda example: example['label'] == label_result).shuffle(seed=random_seed).select(range(range_num))\n\ndef generate_data(data):\n    return data.shuffle(seed=random_seed).map(lambda elem : {'text': generate_prompt(elem)})","metadata":{"execution":{"iopub.status.busy":"2024-05-14T08:58:34.926037Z","iopub.execute_input":"2024-05-14T08:58:34.926391Z","iopub.status.idle":"2024-05-14T08:58:34.932159Z","shell.execute_reply.started":"2024-05-14T08:58:34.926363Z","shell.execute_reply":"2024-05-14T08:58:34.931172Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"The final step related to processing the data is the function *prepare_data*. This function will call the function *prepare_data_train*, in which we will process the data for training, as the name suggests. Here, we obtain both training data and validation data. Essentially, we extract 1000 cases from the original dataset, of which 900 will be used for training and 100 for validation. It is important to mention that we preserved the data balance, ensuring an equal number of positive and negative reviews in both sets. The other component is used solely to test the model before and after training. For this purpose, we followed the same procedure as we did for *DistilBERT* so that we can compare model performances using the same test data.","metadata":{}},{"cell_type":"code","source":"def prepare_data_train(imdb,train_size,eval_size):\n    aux = int((train_size + eval_size) / 2)\n    positive_rows = select(imdb['train'],1,aux)\n    negative_rows = select(imdb['train'],0,aux)\n    \n    half_train_size = int(train_size / 2)\n    half_eval_size = int(eval_size / 2)\n    \n    positive_rows_train = positive_rows.select(indices=range(half_train_size))\n    negative_rows_train = negative_rows.select(indices=range(half_train_size))\n    positive_rows_eval  = positive_rows.select(indices=range(half_train_size, half_train_size + half_eval_size))\n    negative_rows_eval  = negative_rows.select(indices=range(half_train_size, half_train_size + half_eval_size))\n    \n    selected_rows_train = concatenate_datasets([positive_rows_train, negative_rows_train])\n    selected_rows_eval  = concatenate_datasets([positive_rows_eval, negative_rows_eval])\n    \n    data_train = generate_data(selected_rows_train)\n    data_eval  = generate_data(selected_rows_eval)\n    return data_train,data_eval\n\ndef prepare_data_test(df,test_size):\n    limsup = 40000 + test_size\n    X_test = df.iloc[40000:limsup]\n    X_test['text'] = X_test['review']\n    X_test['text'] = X_test['text'].apply(lambda x: generate_test_prompt(x))\n    y_true = list(X_test['sentiment'])\n    return X_test.drop(['sentiment','review'],axis=1), y_true\n\ndef prepare_data(train_size,eval_size,test_size):\n    df = pd.read_csv('movie_data.csv')\n    imdb = load_dataset('imdb')\n    data_train, data_eval = prepare_data_train(imdb,train_size,eval_size)\n    X_test, y_true = prepare_data_test(df,test_size)\n    return data_train, data_eval, X_test, y_true","metadata":{"execution":{"iopub.status.busy":"2024-05-14T08:59:34.645848Z","iopub.execute_input":"2024-05-14T08:59:34.646521Z","iopub.status.idle":"2024-05-14T08:59:34.657975Z","shell.execute_reply.started":"2024-05-14T08:59:34.646487Z","shell.execute_reply":"2024-05-14T08:59:34.657047Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data_train, data_eval, X_test, y_true = prepare_data(900,100,2500)\n#data_train, data_eval, X_test, y_true = prepare_data(50,20,30)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:54:35.631923Z","iopub.execute_input":"2024-05-14T09:54:35.632321Z","iopub.status.idle":"2024-05-14T09:54:41.004488Z","shell.execute_reply.started":"2024-05-14T09:54:35.632294Z","shell.execute_reply":"2024-05-14T09:54:41.003535Z"},"trusted":true},"execution_count":82,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbc5ec28bf6d49839492403f2d5412f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"204ee94169d14767877035075d98139d"}},"metadata":{}}]},{"cell_type":"code","source":"# Debug Messages, comment if not necessary\nprint(data_train)\nprint(data_eval)\nprint(X_test.info())\nprint(len(y_true))","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:54:51.717414Z","iopub.execute_input":"2024-05-14T09:54:51.718184Z","iopub.status.idle":"2024-05-14T09:54:51.728671Z","shell.execute_reply.started":"2024-05-14T09:54:51.718149Z","shell.execute_reply":"2024-05-14T09:54:51.727451Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['text', 'label'],\n    num_rows: 50\n})\nDataset({\n    features: ['text', 'label'],\n    num_rows: 20\n})\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30 entries, 40000 to 40029\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   text    30 non-null     object\ndtypes: object(1)\nmemory usage: 372.0+ bytes\nNone\n30\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Functions for Model Evaluation\n\n\nBefore obtaining the model itself and tuning it, we need to define a method for evaluating it. For that purpose, we have defined multiple functions.\n\nFirstly, we have the functions *accuracy_for_label* and *evaluate*. As their names suggest, the first function returns the accuracy of model answers for each classification (positive or negative), while the second function evaluates the overall model responses.\n\nThe metrics we have utilized include overall accuracy and accuracy for each label (positive and negative reviews).","metadata":{}},{"cell_type":"code","source":"def accuracy_for_label(y_true, y_pred, label):\n    label_indices = [i for i, y in enumerate(y_true) if y == label]\n    label_y_true = [y_true[i] for i in label_indices]\n    label_y_pred = [y_pred[i] for i in label_indices]\n    accuracy = accuracy_score(label_y_true, label_y_pred)\n    return accuracy,int(len(label_y_true)*accuracy),len(label_y_true)\n\ndef evaluate(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Overall Accuracy\n    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n    print(f'Accuracy: {accuracy:.3f}')\n\n    # Accuracy for each label\n    accuracy_negative,correct_negative,total_negative = accuracy_for_label(y_true,y_pred,0)\n    accuracy_positive,correct_positive,total_positive = accuracy_for_label(y_true,y_pred,1)\n\n    print(f'Accuracy for negative reviews: {accuracy_negative:.3f} ({correct_negative},{total_negative})')\n    print(f'Accuracy for positive reviews: {accuracy_positive:.3f} ({correct_positive},{total_positive})')","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:03:12.380177Z","iopub.execute_input":"2024-05-14T09:03:12.381233Z","iopub.status.idle":"2024-05-14T09:03:12.38992Z","shell.execute_reply.started":"2024-05-14T09:03:12.38119Z","shell.execute_reply":"2024-05-14T09:03:12.38895Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Functions for Answer Generation\n\nNext, we have created the functions *generate_response* and *predict*. As their names suggest, the first function provides a single prompt to the model and returns its response. The second function provides all the test prompts to the model and saves the model's responses. We obtain the model's response using the pipeline package. Specifically, we specify that we only want responses with 1 token, so that the model provides only one word. If the model generates a word different from \"positive,\" we consider that the model has evaluated the review as negative, regardless of whether it responded with \"negative\" or something else.","metadata":{}},{"cell_type":"code","source":"def generate_response(prompt,model,tokenizer):\n    pipe = pipeline(task=\"text-generation\",\n                        model=model,\n                        tokenizer=tokenizer,\n                        max_new_tokens = 1,\n                        temperature = 0.0)\n    result = pipe(prompt, pad_token_id=pipe.tokenizer.eos_token_id)\n    return result[0]['generated_text'].split(\"=\")[-1].lower()\n\ndef predict(X_test, model, tokenizer):\n    y_pred = []\n    for i in tqdm(range(len(X_test))):\n        prompt = X_test.iloc[i][\"text\"]\n        answer = generate_response(prompt,model,tokenizer)\n        if \"positive\" in answer:\n            y_pred.append(1)\n        else:\n            y_pred.append(0)\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:03:18.894182Z","iopub.execute_input":"2024-05-14T09:03:18.894691Z","iopub.status.idle":"2024-05-14T09:03:18.902928Z","shell.execute_reply.started":"2024-05-14T09:03:18.894653Z","shell.execute_reply":"2024-05-14T09:03:18.901829Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Mistral Workflow\n\nWe have created a function to retrieve the Mistral model and its tokenizer. As mentioned previously, the model is the Instruct second version of Mistral (`mistralai/Mistral-7B-Instruct-v0.2`).\n\nAn important aspect we've utilized is the package `BitsAndBytesConfig`. This package is useful for configuring the model itself for quantization. Quantization, in general, is a process of reducing the number of bits used to represent data while preserving essential information, resulting in a reduction of model size and an increase in speed (Dettmers, Tim, and Luke Zettlemoyer. 2023. “The Case for 4-Bit Precision: K-Bit Inference Scaling Laws.” https://arxiv.org/abs/2212.09720). Regarding the arguments, *load_in_4bit* is set to True to enable 4-bit quantization. This replaces the Linear layers with FP4/NF4 layers from `bitsandbytes`. The argument *bnb_4bit_use_double_quant* is set to False to prevent re-quantization of quantization constants. *bnb_4bit_quant_type* is set to \"nf4\", where \"nf4\" stands for \"Non-Flowing 4-bit\". This means the model will map the 4-bit quantized values to a discrete set of integers without any fractional parts. Lastly, *bnb_4bit_compute_dtype* is set to `getattr(torch, \"float16\")` to specify the computational type as float bits, thereby increasing computation speed.\n\nWe have used 4 bits instead of 8 bits mainly to reduce the memory size, since at the beginning of the project we were encountering CUDA out of memory errors multiple times. Obviously, this brings some problems such as losing accuracy and precision with the values, as well as limiting the range and 4-bit representations are more susceptible to noise. As for the advantages, the memory size is reduced. It also reduces bandwidth consumption, speeds up computation, and is more energy-efficient.\n\nAfter defining the `BitsAndBytesConfig` object, we retrieve the model itself by passing its name, the `BitsAndBytesConfig` object, and setting *device_map* to \"auto\" so that the model is loaded onto the GPU. Finally, we obtain the model's tokenizer.\n\nThis functionality is encapsulated in the function named `get_model`.","metadata":{}},{"cell_type":"code","source":"def get_model():\n    model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n    compute_dtype = getattr(torch, \"float16\")\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=False,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        quantization_config=bnb_config,\n    )\n    model.config.use_cache = False\n    model.config.pretraining_tp = 1\n    tokenizer = AutoTokenizer.from_pretrained(model_name,\n                                              trust_remote_code=True,\n                                              padding_side=\"left\",\n                                              add_bos_token=True,\n                                              add_eos_token=True,\n                                            )\n    tokenizer.pad_token = tokenizer.eos_token\n    return (model,tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:54:59.94055Z","iopub.execute_input":"2024-05-14T09:54:59.941213Z","iopub.status.idle":"2024-05-14T09:54:59.948451Z","shell.execute_reply.started":"2024-05-14T09:54:59.941179Z","shell.execute_reply":"2024-05-14T09:54:59.947435Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"model,tokenizer = get_model()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:55:00.147305Z","iopub.execute_input":"2024-05-14T09:55:00.14765Z","iopub.status.idle":"2024-05-14T09:55:02.700338Z","shell.execute_reply.started":"2024-05-14T09:55:00.147625Z","shell.execute_reply":"2024-05-14T09:55:02.699054Z"},"trusted":true},"execution_count":86,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model,tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[85], line 10\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m compute_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      5\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mcompute_dtype,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3634\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3630\u001b[0m         device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3631\u001b[0m             key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modules_to_not_convert\n\u001b[1;32m   3632\u001b[0m         }\n\u001b[1;32m   3633\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m-> 3634\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;124;03m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[1;32m   3637\u001b[0m \u001b[38;5;124;03m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[1;32m   3638\u001b[0m \u001b[38;5;124;03m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[1;32m   3639\u001b[0m \u001b[38;5;124;03m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[1;32m   3640\u001b[0m \u001b[38;5;124;03m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[1;32m   3641\u001b[0m \u001b[38;5;124;03m                for more details.\u001b[39;00m\n\u001b[1;32m   3642\u001b[0m \u001b[38;5;124;03m                \"\"\"\u001b[39;00m\n\u001b[1;32m   3643\u001b[0m             )\n\u001b[1;32m   3644\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m device_map_without_lm_head\n\u001b[1;32m   3646\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "],"ename":"ValueError","evalue":"\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ","output_type":"error"}]},{"cell_type":"code","source":"# Base Model Performance. Since this evaluation takes too much time, it is in comments, but uncomment if necessary.\n# y_pred = predict(X_test, model, tokenizer)\n# evaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:05:44.903743Z","iopub.execute_input":"2024-05-14T09:05:44.904081Z","iopub.status.idle":"2024-05-14T09:05:54.954681Z","shell.execute_reply.started":"2024-05-14T09:05:44.904056Z","shell.execute_reply":"2024-05-14T09:05:54.953679Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"100%|██████████| 10/10 [00:10<00:00,  1.00s/it]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.700\nAccuracy for negative reviews: 1.000 (3,3)\nAccuracy for positive reviews: 0.571 (4,7)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"At this point, we've created a function responsible for creating an object that we will use to train the model.\n\nFirstly, we've instantiated a `LoraConfig` object. Lora stands for Low-Rank Adaptation, and it is a PEFT method that decomposes a large matrix into two smaller low-rank matrices in the attention layers. This results in a drastic reduction in the number of parameters that need to be fine-tuned. Regarding the parameters, we've set *lora_alpha* to 16, indicating that the LoRa regularization is set to 16. The *lora_dropout* parameter is set to 0.1, which helps prevent the model from overfitting. The parameter *r* represents the number of multi-head attention the model will use. The *bias* parameter determines whether biases are included in the model's computations. In our case, it is set to None since we do not have biases. The last argument, *task_type*, specifies the type of task the model is trained for, which is set to causal language modeling. This means that the task the model will be trained for is to predict the next word in a sequence given the previous words.\n\nRegarding the training arguments, the first one is *output_dir*, which is simply a Hugging Face project where we want to save the retrained model. Then, we specify the number of epochs (4), the batch size (1), the gradient accumulation (4), the optimizer to use (paged_adamw_32bit), the number of steps to save the checkpoint (0), the number of steps between two logs (25), the learning rate ($2 \\times 10^{-4}$), the weight decay to apply to all layers except all bias and LayerNorm weights (0.001), the usage of floating-point 16-bit values, maximum gradient norm (0.3), the ratio of total training steps used for a linear warmup from 0 to *learning_rate* (0.03), to group together samples of roughly the same length in the training dataset in order to minimize padding applied and be more efficient, the scheduler type to use (cosine), and finally, evaluation is done at the end of each epoch.\n\nAt the end, we use the Supervised Fine-tuning Trainer (SFTT), which will be the trainer object. We pass as arguments our model, the training data, the evaluation data (so that the training process can calculate validation values), the *Lora configuration*, the model *tokenizer*, the training arguments, and we set that the sequences have a maximum length of 512.\n","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    predictions_flat = predictions.flatten()\n    labels_flat = labels.flatten()\n    accuracy = accuracy_score(labels_flat, predictions_flat)\n    precision = precision_score(labels_flat, predictions_flat, average='macro')\n    recall = recall_score(labels_flat, predictions_flat, average='macro')\n    f1 = f1_score(labels_flat, predictions_flat, average='macro')\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1-score': f1\n    }","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:51:40.515829Z","iopub.execute_input":"2024-05-14T09:51:40.516694Z","iopub.status.idle":"2024-05-14T09:51:40.523234Z","shell.execute_reply.started":"2024-05-14T09:51:40.516663Z","shell.execute_reply":"2024-05-14T09:51:40.522307Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"def train_configuration():\n    peft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    training_arguments = TrainingArguments(\n        output_dir=\"mistral_retrained\",\n        num_train_epochs=4,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        optim=\"paged_adamw_32bit\",\n        save_steps=0,\n        logging_steps=25,\n        learning_rate=2e-4,\n        weight_decay=0.001,\n        fp16=True,\n        max_grad_norm=0.3,\n        warmup_ratio=0.03,\n        group_by_length=True,\n        lr_scheduler_type=\"cosine\",\n        report_to=\"tensorboard\",\n        evaluation_strategy=\"epoch\"\n    )\n\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=data_train,\n        eval_dataset=data_eval,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        tokenizer=tokenizer,\n        args=training_arguments,\n        packing=False,\n        max_seq_length=512,\n        compute_metrics=compute_metrics\n    )\n    return trainer","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:51:41.271711Z","iopub.execute_input":"2024-05-14T09:51:41.272509Z","iopub.status.idle":"2024-05-14T09:51:41.279944Z","shell.execute_reply.started":"2024-05-14T09:51:41.272457Z","shell.execute_reply":"2024-05-14T09:51:41.279019Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"## Tuning\n\nSo now we retrain Mistral with our data. The training process took 1 hour and 26 minutes.","metadata":{}},{"cell_type":"code","source":"trainer = train_configuration()\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:51:41.923449Z","iopub.execute_input":"2024-05-14T09:51:41.924119Z","iopub.status.idle":"2024-05-14T09:52:42.862578Z","shell.execute_reply.started":"2024-05-14T09:51:41.924086Z","shell.execute_reply":"2024-05-14T09:52:42.861695Z"},"trusted":true},"execution_count":79,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8/8 00:55, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1-score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>3.079500</td>\n      <td>0.000586</td>\n      <td>0.000835</td>\n      <td>0.000474</td>\n      <td>0.000540</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>2.663174</td>\n      <td>0.002539</td>\n      <td>0.001184</td>\n      <td>0.001093</td>\n      <td>0.000979</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>2.657678</td>\n      <td>0.002539</td>\n      <td>0.001184</td>\n      <td>0.001094</td>\n      <td>0.000979</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=8, training_loss=2.8907713890075684, metrics={'train_runtime': 59.6193, 'train_samples_per_second': 0.671, 'train_steps_per_second': 0.134, 'total_flos': 484679252434944.0, 'train_loss': 2.8907713890075684, 'epoch': 3.2})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Evaluation\n\nNow we need to evaluate our fine tuned Mistral. The evaluation took 1 hour 20 minutes","metadata":{}},{"cell_type":"code","source":"y_pred = predict(X_test, model, tokenizer)\nevaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:52:50.359588Z","iopub.execute_input":"2024-05-14T09:52:50.35999Z","iopub.status.idle":"2024-05-14T09:52:57.617065Z","shell.execute_reply.started":"2024-05-14T09:52:50.359958Z","shell.execute_reply":"2024-05-14T09:52:57.616204Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stderr","text":"100%|██████████| 10/10 [00:07<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.700\nAccuracy for negative reviews: 1.000 (3,3)\nAccuracy for positive reviews: 0.571 (4,7)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Publish\n\nPublish this fine tuned model to hugging face","metadata":{}},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T13:46:59.731375Z","iopub.execute_input":"2024-05-07T13:46:59.731784Z","iopub.status.idle":"2024-05-07T13:47:05.35145Z","shell.execute_reply.started":"2024-05-07T13:46:59.731755Z","shell.execute_reply":"2024-05-07T13:47:05.35053Z"},"trusted":true},"execution_count":null,"outputs":[]}]}