{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/josebambora/mistral-sentimental-analysis?scriptVersionId=176212028\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install -q -U torch=='2.0.0'","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:39:16.183644Z","iopub.execute_input":"2024-05-07T10:39:16.183989Z","iopub.status.idle":"2024-05-07T10:41:15.724907Z","shell.execute_reply.started":"2024-05-07T10:39:16.183962Z","shell.execute_reply":"2024-05-07T10:41:15.723888Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U accelerate=='0.25.0' peft=='0.7.1' bitsandbytes=='0.41.3.post2' transformers=='4.36.1' trl=='0.7.4'","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:41:15.7267Z","iopub.execute_input":"2024-05-07T10:41:15.727075Z","iopub.status.idle":"2024-05-07T10:41:48.159174Z","shell.execute_reply.started":"2024-05-07T10:41:15.727042Z","shell.execute_reply":"2024-05-07T10:41:48.158016Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset, concatenate_datasets\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom transformers import (AutoModelForCausalLM,\n                          AutoTokenizer,\n                          BitsAndBytesConfig,\n                          TrainingArguments,\n                          pipeline,\n                          logging)\nfrom sklearn.metrics import accuracy_score\nfrom datasets import load_dataset\nimport re\nimport requests\nimport gzip\nimport shutil","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:41:48.16052Z","iopub.execute_input":"2024-05-07T10:41:48.160833Z","iopub.status.idle":"2024-05-07T10:42:09.251405Z","shell.execute_reply.started":"2024-05-07T10:41:48.1608Z","shell.execute_reply":"2024-05-07T10:42:09.250622Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-07 10:41:57.832391: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-07 10:41:57.832480: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-07 10:41:57.995166: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"notebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:42:17.093216Z","iopub.execute_input":"2024-05-07T10:42:17.093571Z","iopub.status.idle":"2024-05-07T10:42:17.11528Z","shell.execute_reply.started":"2024-05-07T10:42:17.093542Z","shell.execute_reply":"2024-05-07T10:42:17.114394Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e892269f255c4cd99ff18ff1723e491c"}},"metadata":{}}]},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:42:23.621487Z","iopub.execute_input":"2024-05-07T10:42:23.621898Z","iopub.status.idle":"2024-05-07T10:42:23.627568Z","shell.execute_reply.started":"2024-05-07T10:42:23.621867Z","shell.execute_reply":"2024-05-07T10:42:23.626501Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Prepare the data\n\n- Prepare the IMDb dataset.\n- Format the dataset to facilitate model learning by appending [INST] at the start and [/INST] at the end of each sentence.\n- Allocate 900 cases for training, 100 for validation, and 1000 for testing.","metadata":{}},{"cell_type":"code","source":"def save_data():\n    url = \"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz\"\n    filename = url.split(\"/\")[-1]\n\n    with open(filename, \"wb\") as f:\n        r = requests.get(url)\n        f.write(r.content)\n\n    with gzip.open('movie_data.csv.gz', 'rb') as f_in:\n        with open('movie_data.csv', 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\nsave_data()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:42:26.19967Z","iopub.execute_input":"2024-05-07T10:42:26.200034Z","iopub.status.idle":"2024-05-07T10:42:28.800459Z","shell.execute_reply.started":"2024-05-07T10:42:26.200006Z","shell.execute_reply":"2024-05-07T10:42:28.799398Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def generate_prompt(data_point):\n    label = 'positive'\n    if data_point[\"label\"] != 1:\n        label = 'negative'\n    res = f\"\"\"\n            [INST]Analyze the sentiment of the movie review enclosed in square brackets,\n            determine if it is positive, or negative, and return the answer as\n            the corresponding sentiment label \"positive\" or \"negative\"[/INST]\n\n            [{data_point[\"text\"]}] = {label}\"\"\".strip()\n    return re.sub(r'\\s+', ' ', res)\n\ndef generate_test_prompt(data_point):\n    res = f\"\"\"\n            [INST]Analyze the sentiment of the movie review enclosed in square brackets,\n            determine if it is positive, or negative, and return the answer as\n            the corresponding sentiment label \"positive\" or \"negative\"[/INST]\n\n            [{data_point}] = \"\"\".strip()\n    return re.sub(r'\\s+', ' ', res)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:42:34.763688Z","iopub.execute_input":"2024-05-07T10:42:34.76404Z","iopub.status.idle":"2024-05-07T10:42:34.770461Z","shell.execute_reply.started":"2024-05-07T10:42:34.764015Z","shell.execute_reply":"2024-05-07T10:42:34.769386Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"random_seed = 2000\n\ndef select(data,label_result,range_num):\n    return data.filter(lambda example: example['label'] == label_result).shuffle(seed=random_seed).select(range(range_num))\n\ndef generate_data(data):\n    return data.shuffle(seed=random_seed).map(lambda elem : {'text': generate_prompt(elem)})","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:42:36.071538Z","iopub.execute_input":"2024-05-07T10:42:36.072308Z","iopub.status.idle":"2024-05-07T10:42:36.078156Z","shell.execute_reply.started":"2024-05-07T10:42:36.072276Z","shell.execute_reply":"2024-05-07T10:42:36.077069Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def prepare_data_train(imdb):\n    positive_rows = select(imdb['train'],1,500)\n    negative_rows = select(imdb['train'],0,500)\n    \n    positive_rows_train = positive_rows.select(indices=range(450))\n    negative_rows_train = negative_rows.select(indices=range(450))\n    positive_rows_eval  = positive_rows.select(indices=range(450, 500))\n    negative_rows_eval  = negative_rows.select(indices=range(450, 500))\n    \n    selected_rows_train = concatenate_datasets([positive_rows_train, negative_rows_train])\n    selected_rows_eval  = concatenate_datasets([positive_rows_eval, negative_rows_eval])\n    \n    data_train = generate_data(selected_rows_train)\n    data_eval  = generate_data(selected_rows_eval)\n    return data_train,data_eval\n\ndef prepare_data_test(df):\n    X_test = df.iloc[40000:42500]\n    X_test['text'] = X_test['review']\n    X_test['text'] = X_test['text'].apply(lambda x: generate_test_prompt(x))\n    y_true = list(X_test['sentiment'])\n    return X_test.drop(['sentiment','review'],axis=1), y_true\n\ndef prepare_data():\n    df = pd.read_csv('movie_data.csv')\n    imdb = load_dataset('imdb')\n    data_train, data_eval = prepare_data_train(imdb)\n    X_test, y_true = prepare_data_test(df)\n    return data_train, data_eval, X_test, y_true","metadata":{"execution":{"iopub.status.busy":"2024-05-07T12:17:50.203927Z","iopub.execute_input":"2024-05-07T12:17:50.204288Z","iopub.status.idle":"2024-05-07T12:17:50.214297Z","shell.execute_reply.started":"2024-05-07T12:17:50.204261Z","shell.execute_reply":"2024-05-07T12:17:50.21345Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"data_train, data_eval, X_test, y_true = prepare_data()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T12:17:50.803202Z","iopub.execute_input":"2024-05-07T12:17:50.803803Z","iopub.status.idle":"2024-05-07T12:17:58.607293Z","shell.execute_reply.started":"2024-05-07T12:17:50.803773Z","shell.execute_reply":"2024-05-07T12:17:58.606543Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Debug Messages, uncomment if necessary\n# print(data_train)\n# print(data_eval)\n# print(data_train[0]['text'])\n# print(X_test.info())\n# print(y_true)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:43:17.361006Z","iopub.execute_input":"2024-05-07T10:43:17.361617Z","iopub.status.idle":"2024-05-07T10:43:17.365524Z","shell.execute_reply.started":"2024-05-07T10:43:17.361567Z","shell.execute_reply":"2024-05-07T10:43:17.364565Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Functions for Model Evaluation\n\n- accuracy_for_label: Computes accuracy for positive or negative reviews.\n- evaluate: Computes overall accuracy and accuracy for each label using the previous function.","metadata":{}},{"cell_type":"code","source":"def accuracy_for_label(y_true, y_pred, label):\n    label_indices = [i for i, y in enumerate(y_true) if y == label]\n    label_y_true = [y_true[i] for i in label_indices]\n    label_y_pred = [y_pred[i] for i in label_indices]\n    return accuracy_score(label_y_true, label_y_pred)\n\ndef evaluate(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Overall Accuracy\n    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n    print(f'Accuracy: {accuracy:.3f}')\n\n    # Accuracy for each label\n    accuracy_negative = accuracy_for_label(y_true,y_pred,0)\n    accuracy_positive = accuracy_for_label(y_true,y_pred,1)\n\n    print(f'Accuracy for negative reviews: {accuracy_negative:.3f}')\n    print(f'Accuracy for positive reviews: {accuracy_positive:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:43:19.042848Z","iopub.execute_input":"2024-05-07T10:43:19.043574Z","iopub.status.idle":"2024-05-07T10:43:19.051021Z","shell.execute_reply.started":"2024-05-07T10:43:19.043545Z","shell.execute_reply":"2024-05-07T10:43:19.049992Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Functions for Answer Generation\n\n- generate_response: Retrieves the model's response.\n- predict: Invokes generate_response for each test case.","metadata":{}},{"cell_type":"code","source":"def generate_response(prompt,model,tokenizer):\n    pipe = pipeline(task=\"text-generation\",\n                        model=model,\n                        tokenizer=tokenizer,\n                        max_new_tokens = 1,\n                        temperature = 0.0)\n    result = pipe(prompt, pad_token_id=pipe.tokenizer.eos_token_id)\n    return result[0]['generated_text'].split(\"=\")[-1].lower()\n\ndef predict(X_test, model, tokenizer):\n    y_pred = []\n    for i in tqdm(range(len(X_test))):\n        prompt = X_test.iloc[i][\"text\"]\n        answer = generate_response(prompt,model,tokenizer)\n        if \"positive\" in answer:\n            y_pred.append(1)\n        else:\n            y_pred.append(0)\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:43:21.363149Z","iopub.execute_input":"2024-05-07T10:43:21.363967Z","iopub.status.idle":"2024-05-07T10:43:21.371005Z","shell.execute_reply.started":"2024-05-07T10:43:21.363936Z","shell.execute_reply":"2024-05-07T10:43:21.370014Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Mistral Workflow\n\n- Obtain version Mistral-7B-Instruct-v0.2.\n- Assess the performance of the base model.\n- Train the model using our data.\n- Evaluate the retrained model.","metadata":{}},{"cell_type":"code","source":"def get_model():\n    model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n    compute_dtype = getattr(torch, \"float16\")\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=False,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        quantization_config=bnb_config,\n    )\n    model.config.use_cache = False\n    model.config.pretraining_tp = 1\n    tokenizer = AutoTokenizer.from_pretrained(model_name,\n                                              trust_remote_code=True,\n                                              padding_side=\"left\",\n                                              add_bos_token=True,\n                                              add_eos_token=True,\n                                            )\n    tokenizer.pad_token = tokenizer.eos_token\n    return (model,tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:43:24.381518Z","iopub.execute_input":"2024-05-07T10:43:24.382262Z","iopub.status.idle":"2024-05-07T10:43:24.392638Z","shell.execute_reply.started":"2024-05-07T10:43:24.38221Z","shell.execute_reply":"2024-05-07T10:43:24.391434Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model,tokenizer = get_model()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:43:25.553261Z","iopub.execute_input":"2024-05-07T10:43:25.553634Z","iopub.status.idle":"2024-05-07T10:44:53.334508Z","shell.execute_reply.started":"2024-05-07T10:43:25.553583Z","shell.execute_reply":"2024-05-07T10:44:53.333733Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2e1f767d2cd4a85a29572b063066947"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bff7e0f40d974429a3c003d174e3f7fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82e333392ce04596aae869239ac73b7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a9c02fccaf246e28974bf1208ffe248"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da87a196d20a46938f2e791c3febf125"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21d7e6cf54a44201866b6f6e13bfc35c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"271bcbbd55524fbcb7c50750d9c8db9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e7d4f69ae3242d18faaec043ad8d7ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d130bbebf1f24a7ea38533e6b7aeb945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"711177250c8443188f6769769b746609"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25e0c794713248dfacfa7a7ec9df60b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d759887a7fb64e20af8e06b505748aa2"}},"metadata":{}}]},{"cell_type":"code","source":"# Base Model Performance. Since this evaluation takes too much time, it is in comments, but uncomment if necessary.\n# y_pred = predict(X_test, model, tokenizer)\n# evaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:44:56.768157Z","iopub.execute_input":"2024-05-07T10:44:56.768514Z","iopub.status.idle":"2024-05-07T10:44:56.774547Z","shell.execute_reply.started":"2024-05-07T10:44:56.768486Z","shell.execute_reply":"2024-05-07T10:44:56.773614Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def train_configuration():\n    peft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    training_arguments = TrainingArguments(\n        output_dir=\"mistral_retrained\",\n        num_train_epochs=4,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        optim=\"paged_adamw_32bit\",\n        save_steps=0,\n        logging_steps=25,\n        learning_rate=2e-4,\n        weight_decay=0.001,\n        fp16=True,\n        max_grad_norm=0.3,\n        warmup_ratio=0.03,\n        group_by_length=True,\n        lr_scheduler_type=\"cosine\",\n        report_to=\"tensorboard\",\n        evaluation_strategy=\"epoch\"\n    )\n\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=data_train,\n        eval_dataset=data_eval,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        tokenizer=tokenizer,\n        args=training_arguments,\n        packing=False,\n        max_seq_length=512,\n    )\n    return trainer","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:44:57.927616Z","iopub.execute_input":"2024-05-07T10:44:57.928259Z","iopub.status.idle":"2024-05-07T10:44:57.935382Z","shell.execute_reply.started":"2024-05-07T10:44:57.92823Z","shell.execute_reply":"2024-05-07T10:44:57.934502Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"trainer = train_configuration()\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:45:00.217887Z","iopub.execute_input":"2024-05-07T10:45:00.218241Z","iopub.status.idle":"2024-05-07T12:11:12.386612Z","shell.execute_reply.started":"2024-05-07T10:45:00.218215Z","shell.execute_reply":"2024-05-07T12:11:12.385643Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e878385ea605492aab05900b1ef1b592"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19f29ab3c60b470f8d59f54493568846"}},"metadata":{}},{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [900/900 1:26:00, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.048600</td>\n      <td>2.069510</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.966400</td>\n      <td>2.078540</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.829700</td>\n      <td>2.104064</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.733900</td>\n      <td>2.127548</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=900, training_loss=1.9081769222683376, metrics={'train_runtime': 5169.7089, 'train_samples_per_second': 0.696, 'train_steps_per_second': 0.174, 'total_flos': 5.055962222051328e+16, 'train_loss': 1.9081769222683376, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"y_pred = predict(X_test, model, tokenizer)\nevaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T12:18:07.489502Z","iopub.execute_input":"2024-05-07T12:18:07.490117Z","iopub.status.idle":"2024-05-07T13:38:38.898169Z","shell.execute_reply.started":"2024-05-07T12:18:07.490088Z","shell.execute_reply":"2024-05-07T13:38:38.897242Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [1:20:31<00:00,  1.93s/it]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.964\nAccuracy for negative reviews: 0.970\nAccuracy for positive reviews: 0.959\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T13:46:59.731375Z","iopub.execute_input":"2024-05-07T13:46:59.731784Z","iopub.status.idle":"2024-05-07T13:47:05.35145Z","shell.execute_reply.started":"2024-05-07T13:46:59.731755Z","shell.execute_reply":"2024-05-07T13:47:05.35053Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/109M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc160ab497cd4453b9f6a11c4147b7a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21ef28ef5797445ebeb63847414eff0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.28k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a66dc8a22b3440aba3c93023a3644531"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1715078702.2f49363661b3.34.0:   0%|          | 0.00/11.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6dade313d704961b8603f4f7d2c18d8"}},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/JoseBambora/mistral_retrained/commit/2cbdfb783459feb96c26ffd9057a7aa75ae4eb64', commit_message='End of training', commit_description='', oid='2cbdfb783459feb96c26ffd9057a7aa75ae4eb64', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}